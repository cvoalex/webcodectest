# Go Lipsync Server - Architecture Guide

## Overview

This project provides **three complete server implementations** for real-time lip-sync inference and compositing:

1. **ğŸš€ Monolithic Server** (Recommended for most use cases)
2. **ğŸ”§ Separated Servers** (Compositing + Inference)
3. **ğŸ§ª Legacy Python Server** (Reference implementation)

## Quick Comparison

| Feature | Monolithic | Separated | Python |
|---------|-----------|-----------|--------|
| **Latency** | âœ… ~130ms | ~145ms | ~250ms |
| **Overhead** | âœ… 2-3ms | 10-15ms | High |
| **Deployment** | âœ… 1 binary | 2 binaries | 1 script |
| **Scalability** | Single machine | âœ… Horizontal | Limited |
| **Best for** | âœ… Performance | Distribution | Development |

## Architecture Decision Tree

```
START: What's your primary goal?

â”œâ”€ Maximum Performance? (Lowest latency)
â”‚  â””â”€ ğŸš€ Use MONOLITHIC SERVER
â”‚     Location: go-monolithic-server/
â”‚     Latency: ~130ms
â”‚     Overhead: 2-3ms
â”‚
â”œâ”€ Horizontal Scaling? (Multiple machines)
â”‚  â””â”€ ğŸ”§ Use SEPARATED SERVERS
â”‚     Location: go-compositing-server/ + go-inference-server/
â”‚     Scale: Independent compositing/inference scaling
â”‚     Latency: ~145ms
â”‚
â””â”€ Development/Prototyping?
   â””â”€ ğŸ§ª Use PYTHON SERVER (or Monolithic)
      Location: fast_service/
      Easy to modify, slower performance
```

## Detailed Comparison

### 1. Monolithic Server (Recommended â­)

**Location**: `go-monolithic-server/`

**Architecture**:
```
Single Process:
  Audio Processing â†’ GPU Inference â†’ Compositing â†’ JPEG
  (Direct function calls, zero overhead)
```

**Pros**:
- âœ… **Lowest latency** (~130ms)
- âœ… **Minimal overhead** (2-3ms)
- âœ… **Simplest deployment** (1 binary, 1 config)
- âœ… **Lower cloud costs** (1 server instance)
- âœ… **Easier debugging** (single process)

**Cons**:
- âš ï¸ Can't scale compositing independently
- âš ï¸ Single point of failure

**Use When**:
- Running on single powerful machine
- Latency is critical (real-time requirements)
- Simpler operations preferred
- Cloud cost optimization important

**Quick Start**:
```powershell
cd go-monolithic-server
go build -o monolithic-server.exe ./cmd/server
.\monolithic-server.exe
```

ğŸ“– [Full Documentation](go-monolithic-server/README.md)  
ğŸš€ [Quick Start Guide](go-monolithic-server/QUICKSTART.md)

---

### 2. Separated Servers (Scalable)

**Location**: `go-compositing-server/` + `go-inference-server/`

**Architecture**:
```
Two Processes:
  Client â†’ Compositing Server (gRPC) â†’ Inference Server â†’ GPU
                    â†“
              Compositing CPU Work
```

**Pros**:
- âœ… **Horizontal scaling** (multiple compositing servers)
- âœ… **Resource isolation** (separate CPU/GPU limits)
- âœ… **Independent deployment** (update without full restart)
- âœ… **Load balancing** (distribute across inference servers)

**Cons**:
- âš ï¸ Higher latency (~145ms)
- âš ï¸ More overhead (10-15ms for gRPC)
- âš ï¸ Complex deployment (2 binaries, 2 configs)
- âš ï¸ Higher cloud costs (2 server instances)

**Use When**:
- Need horizontal scaling (>10 req/s)
- Multiple compositing servers share 1 inference server
- Kubernetes/cloud-native deployment
- Different scaling policies needed

**Quick Start**:
```powershell
# Terminal 1: Inference Server
cd go-inference-server
.\inference-server.exe

# Terminal 2: Compositing Server
cd go-compositing-server
.\compositing-server.exe
```

ğŸ“– [Compositing Server Docs](go-compositing-server/README.md)  
ğŸ“– [Inference Server Docs](go-inference-server/README.md)

---

### 3. Python Server (Legacy)

**Location**: `fast_service/`

**Architecture**:
```
Single Python Process:
  FastAPI/gRPC â†’ PyTorch Inference â†’ NumPy Compositing
```

**Pros**:
- âœ… Easy to modify (Python)
- âœ… Good for experimentation
- âœ… Original reference implementation

**Cons**:
- âš ï¸ Slower (~250ms latency)
- âš ï¸ GIL limitations
- âš ï¸ Higher memory usage
- âš ï¸ Slower than Go implementations

**Use When**:
- Rapid prototyping
- Algorithm development
- Need Python ecosystem tools
- Not concerned with performance

**Quick Start**:
```powershell
cd fast_service
python service.py
```

## Performance Benchmarks

### Latency (Batch Size 24)

| Server | Audio | Inference | Composite | Total | Overhead |
|--------|-------|-----------|-----------|-------|----------|
| **Monolithic** | 7ms | 120ms | 2ms | **129ms** | **2ms** |
| Separated | 7ms | 120ms | 2ms | 145ms | 16ms |
| Python | N/A | 180ms | 20ms | 250ms | 50ms |

### Throughput (Requests/Second)

| Server | Single Client | 4 Concurrent | 8 Concurrent |
|--------|--------------|--------------|--------------|
| **Monolithic** | **7.8** | **28** | **52** |
| Separated | 7.0 | 25 | 45 |
| Python | 4.0 | 12 | 18 |

### Memory Usage (Per Model)

| Server | Model Weights | Backgrounds | Runtime | Total |
|--------|--------------|-------------|---------|-------|
| **Monolithic** | 500MB | 250MB | 250MB | **1.0GB** |
| Separated | 500MB + 250MB | - | 300MB | 1.05GB |
| Python | 600MB | 300MB | 400MB | 1.3GB |

## Migration Paths

### From Python â†’ Go Monolithic

**Benefits**: 2Ã— faster, 30% less memory

1. Convert models to ONNX format
2. Build monolithic server
3. Update client to use gRPC (from HTTP)
4. Test with same inputs
5. Deploy and monitor

**Effort**: Medium (model conversion needed)

### From Separated â†’ Monolithic

**Benefits**: 10% faster, simpler deployment

1. Merge config files
2. Update client port (50052 â†’ 50053)
3. Stop both old servers
4. Start monolithic server
5. Test end-to-end

**Effort**: Low (config changes only)

### From Monolithic â†’ Separated

**Benefits**: Enables horizontal scaling

1. Split config into 2 files
2. Start inference server first
3. Start compositing server
4. Update client to compositing server port
5. Test connection health

**Effort**: Low (split and deploy)

## File Structure

```
webcodecstest/
â”œâ”€â”€ go-monolithic-server/          â­ RECOMMENDED
â”‚   â”œâ”€â”€ cmd/server/main.go
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ QUICKSTART.md
â”‚   â””â”€â”€ test_client.go
â”‚
â”œâ”€â”€ go-compositing-server/         (For horizontal scaling)
â”‚   â”œâ”€â”€ cmd/server/main.go
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ test_client.go
â”‚
â”œâ”€â”€ go-inference-server/           (For horizontal scaling)
â”‚   â”œâ”€â”€ cmd/server/main.go
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ audio/ (processing pipeline)
â”‚
â”œâ”€â”€ fast_service/                  (Legacy Python)
â”‚   â”œâ”€â”€ service.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ARCHITECTURE_COMPARISON.md     ğŸ“Š Detailed comparison
â””â”€â”€ ARCHITECTURE_GUIDE.md          ğŸ“– This file
```

## Common Components

All Go servers share:

### Audio Processing Pipeline
- **Location**: `audio/` package
- **Components**:
  - Mel-spectrogram processor (pure Go, gonum/fft)
  - Audio encoder (ONNX Runtime wrapper)
  - Sliding window extraction
- **Performance**: ~7ms per 640ms audio chunk
- **Compatibility**: Validated against Python reference (<0.001 error)

### Model Registry
- **Location**: `registry/` package
- **Features**:
  - Dynamic model loading/unloading
  - LFU eviction policy
  - Multi-GPU support
  - Memory tracking
  - Usage statistics

### ONNX Inference
- **Location**: `lipsyncinfer/` package
- **Runtime**: ONNX Runtime 1.22.0 with CUDA
- **Optimizations**:
  - Pre-allocated tensors
  - Zero-copy operations
  - GPU memory pooling
  - Batch processing

## Configuration

All servers use YAML configuration:

```yaml
server:
  port: ":5005X"              # 50053 (mono), 50052 (comp), 50051 (inf)
  max_message_size_mb: 100

gpus:
  enabled: true
  count: 1
  memory_gb_per_gpu: 24

models:
  sanders:
    model_path: "sanders/checkpoint/model_best.onnx"
    background_dir: "sanders/frames"  # Monolithic/Compositing only
    crop_rects_path: "sanders/crop_rects.json"
    num_frames: 523
    preload_backgrounds: true
```

## Testing

Each server includes a test client:

```powershell
# Build test client
go build -o test-client.exe test_client.go

# Run test
.\test-client.exe
```

**Test features**:
- Loads real audio from `aud.wav`
- Processes raw PCM audio (16kHz mono)
- Sends batches of 24 frames
- Saves JPEG output to `test_output/`
- Reports detailed performance metrics

## Monitoring

All servers provide:

1. **Health Check RPC**
   ```
   Health() â†’ { healthy, loaded_models, gpu_info }
   ```

2. **Performance Logging**
   ```
   ğŸµ Audio processing: 7.45ms
   âš¡ Inference: 120.28ms
   ğŸ¨ Compositing: 2.15ms
   ```

3. **Model Statistics**
   ```
   GetModelStats() â†’ usage counts, avg latency, memory
   ```

## Recommendations

### For Production

| Scenario | Recommended | Reason |
|----------|------------|--------|
| **Single cloud VM** | Monolithic | Lowest latency, lower cost |
| **Edge device** | Monolithic | Resource constrained |
| **Kubernetes cluster** | Separated | Better k8s integration |
| **Multi-region** | Separated | Distributed inference |
| **Development** | Monolithic | Simpler debugging |

### For Scaling

| Load | Architecture | Notes |
|------|-------------|-------|
| **< 10 req/s** | Monolithic | Single server sufficient |
| **10-50 req/s** | Separated (1 inf + 5 comp) | Scale compositing |
| **> 50 req/s** | Separated (Multi-GPU) | Multiple inference servers |

## Getting Started

**Recommended path for new users:**

1. **Start with Monolithic** (fastest to get running)
   ```powershell
   cd go-monolithic-server
   go build -o monolithic-server.exe ./cmd/server
   .\monolithic-server.exe
   ```

2. **Run the test client**
   ```powershell
   go build -o test-client.exe test_client.go
   .\test-client.exe
   ```

3. **Verify performance** (should see ~130ms latency)

4. **If you need scaling later**, migrate to Separated architecture

## Documentation

- ğŸ“– [Monolithic Server README](go-monolithic-server/README.md)
- ğŸš€ [Quick Start Guide](go-monolithic-server/QUICKSTART.md)
- ğŸ“Š [Architecture Comparison](ARCHITECTURE_COMPARISON.md)
- ğŸµ [Audio Integration](go-inference-server/AUDIO_INTEGRATION.md)
- ğŸ“ [Test Client Guide](go-compositing-server/TEST_CLIENT_AUDIO_UPDATE.md)

## Support

Need help choosing? Ask yourself:

1. **Do you need >10 req/s?**
   - No â†’ **Monolithic**
   - Yes â†’ Continue to #2

2. **Do you need independent scaling of compositing vs inference?**
   - No â†’ **Monolithic**
   - Yes â†’ **Separated**

3. **Are you running on Kubernetes?**
   - No â†’ **Monolithic**
   - Yes â†’ **Separated**

**When in doubt, start with Monolithic.** It's faster, simpler, and you can migrate later if needed.

---

**All architectures are production-ready!** Choose based on your scaling needs. ğŸš€
