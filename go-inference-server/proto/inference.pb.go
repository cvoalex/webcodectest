// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.10
// 	protoc        v5.28.3
// source: proto/inference.proto

package proto

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type InferBatchRequest struct {
	state        protoimpl.MessageState `protogen:"open.v1"`
	ModelId      string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`                // Model to use (e.g., "sanders", "bob")
	VisualFrames []byte                 `protobuf:"bytes,2,opt,name=visual_frames,json=visualFrames,proto3" json:"visual_frames,omitempty"` // 6*320*320 float32 per frame (batch_size frames)
	// Audio input - ONE OF the following:
	RawAudio      []byte `protobuf:"bytes,3,opt,name=raw_audio,json=rawAudio,proto3" json:"raw_audio,omitempty"`                // Raw PCM audio bytes (640ms @ 16kHz = 10,240 samples)
	AudioFeatures []byte `protobuf:"bytes,4,opt,name=audio_features,json=audioFeatures,proto3" json:"audio_features,omitempty"` // Pre-computed audio features (DEPRECATED - for backward compat)
	BatchSize     int32  `protobuf:"varint,5,opt,name=batch_size,json=batchSize,proto3" json:"batch_size,omitempty"`            // Number of frames in batch (1-25)
	SampleRate    int32  `protobuf:"varint,6,opt,name=sample_rate,json=sampleRate,proto3" json:"sample_rate,omitempty"`         // Audio sample rate (default: 16000)
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferBatchRequest) Reset() {
	*x = InferBatchRequest{}
	mi := &file_proto_inference_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferBatchRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferBatchRequest) ProtoMessage() {}

func (x *InferBatchRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferBatchRequest.ProtoReflect.Descriptor instead.
func (*InferBatchRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{0}
}

func (x *InferBatchRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *InferBatchRequest) GetVisualFrames() []byte {
	if x != nil {
		return x.VisualFrames
	}
	return nil
}

func (x *InferBatchRequest) GetRawAudio() []byte {
	if x != nil {
		return x.RawAudio
	}
	return nil
}

func (x *InferBatchRequest) GetAudioFeatures() []byte {
	if x != nil {
		return x.AudioFeatures
	}
	return nil
}

func (x *InferBatchRequest) GetBatchSize() int32 {
	if x != nil {
		return x.BatchSize
	}
	return 0
}

func (x *InferBatchRequest) GetSampleRate() int32 {
	if x != nil {
		return x.SampleRate
	}
	return 0
}

type InferBatchResponse struct {
	state             protoimpl.MessageState `protogen:"open.v1"`
	Outputs           []*RawMouthRegion      `protobuf:"bytes,1,rep,name=outputs,proto3" json:"outputs,omitempty"`                                                  // Raw 320×320×3 float32 mouth regions
	InferenceTimeMs   float32                `protobuf:"fixed32,2,opt,name=inference_time_ms,json=inferenceTimeMs,proto3" json:"inference_time_ms,omitempty"`       // GPU inference time only
	AudioProcessingMs float32                `protobuf:"fixed32,3,opt,name=audio_processing_ms,json=audioProcessingMs,proto3" json:"audio_processing_ms,omitempty"` // Audio encoding time (mel-spec + audio encoder)
	Success           bool                   `protobuf:"varint,4,opt,name=success,proto3" json:"success,omitempty"`
	Error             string                 `protobuf:"bytes,5,opt,name=error,proto3" json:"error,omitempty"`
	WorkerId          int32                  `protobuf:"varint,6,opt,name=worker_id,json=workerId,proto3" json:"worker_id,omitempty"` // Which worker processed this
	GpuId             int32                  `protobuf:"varint,7,opt,name=gpu_id,json=gpuId,proto3" json:"gpu_id,omitempty"`          // Which GPU processed this
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *InferBatchResponse) Reset() {
	*x = InferBatchResponse{}
	mi := &file_proto_inference_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferBatchResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferBatchResponse) ProtoMessage() {}

func (x *InferBatchResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferBatchResponse.ProtoReflect.Descriptor instead.
func (*InferBatchResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{1}
}

func (x *InferBatchResponse) GetOutputs() []*RawMouthRegion {
	if x != nil {
		return x.Outputs
	}
	return nil
}

func (x *InferBatchResponse) GetInferenceTimeMs() float32 {
	if x != nil {
		return x.InferenceTimeMs
	}
	return 0
}

func (x *InferBatchResponse) GetAudioProcessingMs() float32 {
	if x != nil {
		return x.AudioProcessingMs
	}
	return 0
}

func (x *InferBatchResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *InferBatchResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

func (x *InferBatchResponse) GetWorkerId() int32 {
	if x != nil {
		return x.WorkerId
	}
	return 0
}

func (x *InferBatchResponse) GetGpuId() int32 {
	if x != nil {
		return x.GpuId
	}
	return 0
}

type RawMouthRegion struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Data          []byte                 `protobuf:"bytes,1,opt,name=data,proto3" json:"data,omitempty"` // 3*320*320 float32 array (307,200 floats = 1.2MB per frame)
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RawMouthRegion) Reset() {
	*x = RawMouthRegion{}
	mi := &file_proto_inference_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RawMouthRegion) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RawMouthRegion) ProtoMessage() {}

func (x *RawMouthRegion) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RawMouthRegion.ProtoReflect.Descriptor instead.
func (*RawMouthRegion) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{2}
}

func (x *RawMouthRegion) GetData() []byte {
	if x != nil {
		return x.Data
	}
	return nil
}

type ListModelsRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsRequest) Reset() {
	*x = ListModelsRequest{}
	mi := &file_proto_inference_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsRequest) ProtoMessage() {}

func (x *ListModelsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsRequest.ProtoReflect.Descriptor instead.
func (*ListModelsRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{3}
}

type ListModelsResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Models        []*ModelInfo           `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsResponse) Reset() {
	*x = ListModelsResponse{}
	mi := &file_proto_inference_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsResponse) ProtoMessage() {}

func (x *ListModelsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsResponse.ProtoReflect.Descriptor instead.
func (*ListModelsResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{4}
}

func (x *ListModelsResponse) GetModels() []*ModelInfo {
	if x != nil {
		return x.Models
	}
	return nil
}

type LoadModelRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	ForceReload   bool                   `protobuf:"varint,2,opt,name=force_reload,json=forceReload,proto3" json:"force_reload,omitempty"` // Unload and reload if already loaded
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LoadModelRequest) Reset() {
	*x = LoadModelRequest{}
	mi := &file_proto_inference_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LoadModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LoadModelRequest) ProtoMessage() {}

func (x *LoadModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LoadModelRequest.ProtoReflect.Descriptor instead.
func (*LoadModelRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{5}
}

func (x *LoadModelRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *LoadModelRequest) GetForceReload() bool {
	if x != nil {
		return x.ForceReload
	}
	return false
}

type LoadModelResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Success       bool                   `protobuf:"varint,1,opt,name=success,proto3" json:"success,omitempty"`
	Error         string                 `protobuf:"bytes,2,opt,name=error,proto3" json:"error,omitempty"`
	LoadTimeMs    float32                `protobuf:"fixed32,3,opt,name=load_time_ms,json=loadTimeMs,proto3" json:"load_time_ms,omitempty"`
	Stats         *ModelStats            `protobuf:"bytes,4,opt,name=stats,proto3" json:"stats,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LoadModelResponse) Reset() {
	*x = LoadModelResponse{}
	mi := &file_proto_inference_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LoadModelResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LoadModelResponse) ProtoMessage() {}

func (x *LoadModelResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LoadModelResponse.ProtoReflect.Descriptor instead.
func (*LoadModelResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{6}
}

func (x *LoadModelResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *LoadModelResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

func (x *LoadModelResponse) GetLoadTimeMs() float32 {
	if x != nil {
		return x.LoadTimeMs
	}
	return 0
}

func (x *LoadModelResponse) GetStats() *ModelStats {
	if x != nil {
		return x.Stats
	}
	return nil
}

type UnloadModelRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UnloadModelRequest) Reset() {
	*x = UnloadModelRequest{}
	mi := &file_proto_inference_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UnloadModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UnloadModelRequest) ProtoMessage() {}

func (x *UnloadModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UnloadModelRequest.ProtoReflect.Descriptor instead.
func (*UnloadModelRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{7}
}

func (x *UnloadModelRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

type UnloadModelResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Success       bool                   `protobuf:"varint,1,opt,name=success,proto3" json:"success,omitempty"`
	Error         string                 `protobuf:"bytes,2,opt,name=error,proto3" json:"error,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *UnloadModelResponse) Reset() {
	*x = UnloadModelResponse{}
	mi := &file_proto_inference_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *UnloadModelResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*UnloadModelResponse) ProtoMessage() {}

func (x *UnloadModelResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use UnloadModelResponse.ProtoReflect.Descriptor instead.
func (*UnloadModelResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{8}
}

func (x *UnloadModelResponse) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

func (x *UnloadModelResponse) GetError() string {
	if x != nil {
		return x.Error
	}
	return ""
}

type GetModelStatsRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"` // Empty string = all models
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetModelStatsRequest) Reset() {
	*x = GetModelStatsRequest{}
	mi := &file_proto_inference_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetModelStatsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetModelStatsRequest) ProtoMessage() {}

func (x *GetModelStatsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetModelStatsRequest.ProtoReflect.Descriptor instead.
func (*GetModelStatsRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{9}
}

func (x *GetModelStatsRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

type GetModelStatsResponse struct {
	state            protoimpl.MessageState `protogen:"open.v1"`
	Models           []*ModelInfo           `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	MaxModels        int32                  `protobuf:"varint,2,opt,name=max_models,json=maxModels,proto3" json:"max_models,omitempty"`
	LoadedModels     int32                  `protobuf:"varint,3,opt,name=loaded_models,json=loadedModels,proto3" json:"loaded_models,omitempty"`
	TotalMemoryBytes int64                  `protobuf:"varint,4,opt,name=total_memory_bytes,json=totalMemoryBytes,proto3" json:"total_memory_bytes,omitempty"`
	MaxMemoryBytes   int64                  `protobuf:"varint,5,opt,name=max_memory_bytes,json=maxMemoryBytes,proto3" json:"max_memory_bytes,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *GetModelStatsResponse) Reset() {
	*x = GetModelStatsResponse{}
	mi := &file_proto_inference_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetModelStatsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetModelStatsResponse) ProtoMessage() {}

func (x *GetModelStatsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetModelStatsResponse.ProtoReflect.Descriptor instead.
func (*GetModelStatsResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{10}
}

func (x *GetModelStatsResponse) GetModels() []*ModelInfo {
	if x != nil {
		return x.Models
	}
	return nil
}

func (x *GetModelStatsResponse) GetMaxModels() int32 {
	if x != nil {
		return x.MaxModels
	}
	return 0
}

func (x *GetModelStatsResponse) GetLoadedModels() int32 {
	if x != nil {
		return x.LoadedModels
	}
	return 0
}

func (x *GetModelStatsResponse) GetTotalMemoryBytes() int64 {
	if x != nil {
		return x.TotalMemoryBytes
	}
	return 0
}

func (x *GetModelStatsResponse) GetMaxMemoryBytes() int64 {
	if x != nil {
		return x.MaxMemoryBytes
	}
	return 0
}

type HealthRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *HealthRequest) Reset() {
	*x = HealthRequest{}
	mi := &file_proto_inference_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *HealthRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*HealthRequest) ProtoMessage() {}

func (x *HealthRequest) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use HealthRequest.ProtoReflect.Descriptor instead.
func (*HealthRequest) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{11}
}

type HealthResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Healthy       bool                   `protobuf:"varint,1,opt,name=healthy,proto3" json:"healthy,omitempty"`
	CudaAvailable bool                   `protobuf:"varint,2,opt,name=cuda_available,json=cudaAvailable,proto3" json:"cuda_available,omitempty"`
	LoadedModels  int32                  `protobuf:"varint,3,opt,name=loaded_models,json=loadedModels,proto3" json:"loaded_models,omitempty"`
	MaxModels     int32                  `protobuf:"varint,4,opt,name=max_models,json=maxModels,proto3" json:"max_models,omitempty"`
	GpuCount      int32                  `protobuf:"varint,5,opt,name=gpu_count,json=gpuCount,proto3" json:"gpu_count,omitempty"`
	Version       string                 `protobuf:"bytes,6,opt,name=version,proto3" json:"version,omitempty"`
	Gpus          []*GPUInfo             `protobuf:"bytes,7,rep,name=gpus,proto3" json:"gpus,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *HealthResponse) Reset() {
	*x = HealthResponse{}
	mi := &file_proto_inference_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *HealthResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*HealthResponse) ProtoMessage() {}

func (x *HealthResponse) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use HealthResponse.ProtoReflect.Descriptor instead.
func (*HealthResponse) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{12}
}

func (x *HealthResponse) GetHealthy() bool {
	if x != nil {
		return x.Healthy
	}
	return false
}

func (x *HealthResponse) GetCudaAvailable() bool {
	if x != nil {
		return x.CudaAvailable
	}
	return false
}

func (x *HealthResponse) GetLoadedModels() int32 {
	if x != nil {
		return x.LoadedModels
	}
	return 0
}

func (x *HealthResponse) GetMaxModels() int32 {
	if x != nil {
		return x.MaxModels
	}
	return 0
}

func (x *HealthResponse) GetGpuCount() int32 {
	if x != nil {
		return x.GpuCount
	}
	return 0
}

func (x *HealthResponse) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

func (x *HealthResponse) GetGpus() []*GPUInfo {
	if x != nil {
		return x.Gpus
	}
	return nil
}

type GPUInfo struct {
	state            protoimpl.MessageState `protogen:"open.v1"`
	GpuId            int32                  `protobuf:"varint,1,opt,name=gpu_id,json=gpuId,proto3" json:"gpu_id,omitempty"`
	Name             string                 `protobuf:"bytes,2,opt,name=name,proto3" json:"name,omitempty"`
	TotalMemoryBytes int64                  `protobuf:"varint,3,opt,name=total_memory_bytes,json=totalMemoryBytes,proto3" json:"total_memory_bytes,omitempty"`
	UsedMemoryBytes  int64                  `protobuf:"varint,4,opt,name=used_memory_bytes,json=usedMemoryBytes,proto3" json:"used_memory_bytes,omitempty"`
	LoadedModels     int32                  `protobuf:"varint,5,opt,name=loaded_models,json=loadedModels,proto3" json:"loaded_models,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *GPUInfo) Reset() {
	*x = GPUInfo{}
	mi := &file_proto_inference_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GPUInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GPUInfo) ProtoMessage() {}

func (x *GPUInfo) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GPUInfo.ProtoReflect.Descriptor instead.
func (*GPUInfo) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{13}
}

func (x *GPUInfo) GetGpuId() int32 {
	if x != nil {
		return x.GpuId
	}
	return 0
}

func (x *GPUInfo) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *GPUInfo) GetTotalMemoryBytes() int64 {
	if x != nil {
		return x.TotalMemoryBytes
	}
	return 0
}

func (x *GPUInfo) GetUsedMemoryBytes() int64 {
	if x != nil {
		return x.UsedMemoryBytes
	}
	return 0
}

func (x *GPUInfo) GetLoadedModels() int32 {
	if x != nil {
		return x.LoadedModels
	}
	return 0
}

type ModelInfo struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	ModelId       string                 `protobuf:"bytes,1,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	Loaded        bool                   `protobuf:"varint,2,opt,name=loaded,proto3" json:"loaded,omitempty"`
	ModelPath     string                 `protobuf:"bytes,3,opt,name=model_path,json=modelPath,proto3" json:"model_path,omitempty"`
	Stats         *ModelStats            `protobuf:"bytes,4,opt,name=stats,proto3" json:"stats,omitempty"`
	GpuId         int32                  `protobuf:"varint,5,opt,name=gpu_id,json=gpuId,proto3" json:"gpu_id,omitempty"` // Which GPU this model is loaded on
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInfo) Reset() {
	*x = ModelInfo{}
	mi := &file_proto_inference_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInfo) ProtoMessage() {}

func (x *ModelInfo) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInfo.ProtoReflect.Descriptor instead.
func (*ModelInfo) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{14}
}

func (x *ModelInfo) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *ModelInfo) GetLoaded() bool {
	if x != nil {
		return x.Loaded
	}
	return false
}

func (x *ModelInfo) GetModelPath() string {
	if x != nil {
		return x.ModelPath
	}
	return ""
}

func (x *ModelInfo) GetStats() *ModelStats {
	if x != nil {
		return x.Stats
	}
	return nil
}

func (x *ModelInfo) GetGpuId() int32 {
	if x != nil {
		return x.GpuId
	}
	return 0
}

type ModelStats struct {
	state                protoimpl.MessageState `protogen:"open.v1"`
	UsageCount           int64                  `protobuf:"varint,1,opt,name=usage_count,json=usageCount,proto3" json:"usage_count,omitempty"`
	LastUsedUnixMs       int64                  `protobuf:"varint,2,opt,name=last_used_unix_ms,json=lastUsedUnixMs,proto3" json:"last_used_unix_ms,omitempty"`
	TotalInferenceTimeMs float64                `protobuf:"fixed64,3,opt,name=total_inference_time_ms,json=totalInferenceTimeMs,proto3" json:"total_inference_time_ms,omitempty"`
	MemoryBytes          int64                  `protobuf:"varint,4,opt,name=memory_bytes,json=memoryBytes,proto3" json:"memory_bytes,omitempty"`
	LoadedUnixMs         int64                  `protobuf:"varint,5,opt,name=loaded_unix_ms,json=loadedUnixMs,proto3" json:"loaded_unix_ms,omitempty"`
	unknownFields        protoimpl.UnknownFields
	sizeCache            protoimpl.SizeCache
}

func (x *ModelStats) Reset() {
	*x = ModelStats{}
	mi := &file_proto_inference_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelStats) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelStats) ProtoMessage() {}

func (x *ModelStats) ProtoReflect() protoreflect.Message {
	mi := &file_proto_inference_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelStats.ProtoReflect.Descriptor instead.
func (*ModelStats) Descriptor() ([]byte, []int) {
	return file_proto_inference_proto_rawDescGZIP(), []int{15}
}

func (x *ModelStats) GetUsageCount() int64 {
	if x != nil {
		return x.UsageCount
	}
	return 0
}

func (x *ModelStats) GetLastUsedUnixMs() int64 {
	if x != nil {
		return x.LastUsedUnixMs
	}
	return 0
}

func (x *ModelStats) GetTotalInferenceTimeMs() float64 {
	if x != nil {
		return x.TotalInferenceTimeMs
	}
	return 0
}

func (x *ModelStats) GetMemoryBytes() int64 {
	if x != nil {
		return x.MemoryBytes
	}
	return 0
}

func (x *ModelStats) GetLoadedUnixMs() int64 {
	if x != nil {
		return x.LoadedUnixMs
	}
	return 0
}

var File_proto_inference_proto protoreflect.FileDescriptor

const file_proto_inference_proto_rawDesc = "" +
	"\n" +
	"\x15proto/inference.proto\x12\tinference\"\xd7\x01\n" +
	"\x11InferBatchRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x12#\n" +
	"\rvisual_frames\x18\x02 \x01(\fR\fvisualFrames\x12\x1b\n" +
	"\traw_audio\x18\x03 \x01(\fR\brawAudio\x12%\n" +
	"\x0eaudio_features\x18\x04 \x01(\fR\raudioFeatures\x12\x1d\n" +
	"\n" +
	"batch_size\x18\x05 \x01(\x05R\tbatchSize\x12\x1f\n" +
	"\vsample_rate\x18\x06 \x01(\x05R\n" +
	"sampleRate\"\x89\x02\n" +
	"\x12InferBatchResponse\x123\n" +
	"\aoutputs\x18\x01 \x03(\v2\x19.inference.RawMouthRegionR\aoutputs\x12*\n" +
	"\x11inference_time_ms\x18\x02 \x01(\x02R\x0finferenceTimeMs\x12.\n" +
	"\x13audio_processing_ms\x18\x03 \x01(\x02R\x11audioProcessingMs\x12\x18\n" +
	"\asuccess\x18\x04 \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\x05 \x01(\tR\x05error\x12\x1b\n" +
	"\tworker_id\x18\x06 \x01(\x05R\bworkerId\x12\x15\n" +
	"\x06gpu_id\x18\a \x01(\x05R\x05gpuId\"$\n" +
	"\x0eRawMouthRegion\x12\x12\n" +
	"\x04data\x18\x01 \x01(\fR\x04data\"\x13\n" +
	"\x11ListModelsRequest\"B\n" +
	"\x12ListModelsResponse\x12,\n" +
	"\x06models\x18\x01 \x03(\v2\x14.inference.ModelInfoR\x06models\"P\n" +
	"\x10LoadModelRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x12!\n" +
	"\fforce_reload\x18\x02 \x01(\bR\vforceReload\"\x92\x01\n" +
	"\x11LoadModelResponse\x12\x18\n" +
	"\asuccess\x18\x01 \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\x02 \x01(\tR\x05error\x12 \n" +
	"\fload_time_ms\x18\x03 \x01(\x02R\n" +
	"loadTimeMs\x12+\n" +
	"\x05stats\x18\x04 \x01(\v2\x15.inference.ModelStatsR\x05stats\"/\n" +
	"\x12UnloadModelRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\"E\n" +
	"\x13UnloadModelResponse\x12\x18\n" +
	"\asuccess\x18\x01 \x01(\bR\asuccess\x12\x14\n" +
	"\x05error\x18\x02 \x01(\tR\x05error\"1\n" +
	"\x14GetModelStatsRequest\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\"\xe1\x01\n" +
	"\x15GetModelStatsResponse\x12,\n" +
	"\x06models\x18\x01 \x03(\v2\x14.inference.ModelInfoR\x06models\x12\x1d\n" +
	"\n" +
	"max_models\x18\x02 \x01(\x05R\tmaxModels\x12#\n" +
	"\rloaded_models\x18\x03 \x01(\x05R\floadedModels\x12,\n" +
	"\x12total_memory_bytes\x18\x04 \x01(\x03R\x10totalMemoryBytes\x12(\n" +
	"\x10max_memory_bytes\x18\x05 \x01(\x03R\x0emaxMemoryBytes\"\x0f\n" +
	"\rHealthRequest\"\xf4\x01\n" +
	"\x0eHealthResponse\x12\x18\n" +
	"\ahealthy\x18\x01 \x01(\bR\ahealthy\x12%\n" +
	"\x0ecuda_available\x18\x02 \x01(\bR\rcudaAvailable\x12#\n" +
	"\rloaded_models\x18\x03 \x01(\x05R\floadedModels\x12\x1d\n" +
	"\n" +
	"max_models\x18\x04 \x01(\x05R\tmaxModels\x12\x1b\n" +
	"\tgpu_count\x18\x05 \x01(\x05R\bgpuCount\x12\x18\n" +
	"\aversion\x18\x06 \x01(\tR\aversion\x12&\n" +
	"\x04gpus\x18\a \x03(\v2\x12.inference.GPUInfoR\x04gpus\"\xb3\x01\n" +
	"\aGPUInfo\x12\x15\n" +
	"\x06gpu_id\x18\x01 \x01(\x05R\x05gpuId\x12\x12\n" +
	"\x04name\x18\x02 \x01(\tR\x04name\x12,\n" +
	"\x12total_memory_bytes\x18\x03 \x01(\x03R\x10totalMemoryBytes\x12*\n" +
	"\x11used_memory_bytes\x18\x04 \x01(\x03R\x0fusedMemoryBytes\x12#\n" +
	"\rloaded_models\x18\x05 \x01(\x05R\floadedModels\"\xa1\x01\n" +
	"\tModelInfo\x12\x19\n" +
	"\bmodel_id\x18\x01 \x01(\tR\amodelId\x12\x16\n" +
	"\x06loaded\x18\x02 \x01(\bR\x06loaded\x12\x1d\n" +
	"\n" +
	"model_path\x18\x03 \x01(\tR\tmodelPath\x12+\n" +
	"\x05stats\x18\x04 \x01(\v2\x15.inference.ModelStatsR\x05stats\x12\x15\n" +
	"\x06gpu_id\x18\x05 \x01(\x05R\x05gpuId\"\xd8\x01\n" +
	"\n" +
	"ModelStats\x12\x1f\n" +
	"\vusage_count\x18\x01 \x01(\x03R\n" +
	"usageCount\x12)\n" +
	"\x11last_used_unix_ms\x18\x02 \x01(\x03R\x0elastUsedUnixMs\x125\n" +
	"\x17total_inference_time_ms\x18\x03 \x01(\x01R\x14totalInferenceTimeMs\x12!\n" +
	"\fmemory_bytes\x18\x04 \x01(\x03R\vmemoryBytes\x12$\n" +
	"\x0eloaded_unix_ms\x18\x05 \x01(\x03R\floadedUnixMs2\xd1\x03\n" +
	"\x10InferenceService\x12I\n" +
	"\n" +
	"InferBatch\x12\x1c.inference.InferBatchRequest\x1a\x1d.inference.InferBatchResponse\x12I\n" +
	"\n" +
	"ListModels\x12\x1c.inference.ListModelsRequest\x1a\x1d.inference.ListModelsResponse\x12F\n" +
	"\tLoadModel\x12\x1b.inference.LoadModelRequest\x1a\x1c.inference.LoadModelResponse\x12L\n" +
	"\vUnloadModel\x12\x1d.inference.UnloadModelRequest\x1a\x1e.inference.UnloadModelResponse\x12R\n" +
	"\rGetModelStats\x12\x1f.inference.GetModelStatsRequest\x1a .inference.GetModelStatsResponse\x12=\n" +
	"\x06Health\x12\x18.inference.HealthRequest\x1a\x19.inference.HealthResponseB\x1bZ\x19go-inference-server/protob\x06proto3"

var (
	file_proto_inference_proto_rawDescOnce sync.Once
	file_proto_inference_proto_rawDescData []byte
)

func file_proto_inference_proto_rawDescGZIP() []byte {
	file_proto_inference_proto_rawDescOnce.Do(func() {
		file_proto_inference_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_proto_inference_proto_rawDesc), len(file_proto_inference_proto_rawDesc)))
	})
	return file_proto_inference_proto_rawDescData
}

var file_proto_inference_proto_msgTypes = make([]protoimpl.MessageInfo, 16)
var file_proto_inference_proto_goTypes = []any{
	(*InferBatchRequest)(nil),     // 0: inference.InferBatchRequest
	(*InferBatchResponse)(nil),    // 1: inference.InferBatchResponse
	(*RawMouthRegion)(nil),        // 2: inference.RawMouthRegion
	(*ListModelsRequest)(nil),     // 3: inference.ListModelsRequest
	(*ListModelsResponse)(nil),    // 4: inference.ListModelsResponse
	(*LoadModelRequest)(nil),      // 5: inference.LoadModelRequest
	(*LoadModelResponse)(nil),     // 6: inference.LoadModelResponse
	(*UnloadModelRequest)(nil),    // 7: inference.UnloadModelRequest
	(*UnloadModelResponse)(nil),   // 8: inference.UnloadModelResponse
	(*GetModelStatsRequest)(nil),  // 9: inference.GetModelStatsRequest
	(*GetModelStatsResponse)(nil), // 10: inference.GetModelStatsResponse
	(*HealthRequest)(nil),         // 11: inference.HealthRequest
	(*HealthResponse)(nil),        // 12: inference.HealthResponse
	(*GPUInfo)(nil),               // 13: inference.GPUInfo
	(*ModelInfo)(nil),             // 14: inference.ModelInfo
	(*ModelStats)(nil),            // 15: inference.ModelStats
}
var file_proto_inference_proto_depIdxs = []int32{
	2,  // 0: inference.InferBatchResponse.outputs:type_name -> inference.RawMouthRegion
	14, // 1: inference.ListModelsResponse.models:type_name -> inference.ModelInfo
	15, // 2: inference.LoadModelResponse.stats:type_name -> inference.ModelStats
	14, // 3: inference.GetModelStatsResponse.models:type_name -> inference.ModelInfo
	13, // 4: inference.HealthResponse.gpus:type_name -> inference.GPUInfo
	15, // 5: inference.ModelInfo.stats:type_name -> inference.ModelStats
	0,  // 6: inference.InferenceService.InferBatch:input_type -> inference.InferBatchRequest
	3,  // 7: inference.InferenceService.ListModels:input_type -> inference.ListModelsRequest
	5,  // 8: inference.InferenceService.LoadModel:input_type -> inference.LoadModelRequest
	7,  // 9: inference.InferenceService.UnloadModel:input_type -> inference.UnloadModelRequest
	9,  // 10: inference.InferenceService.GetModelStats:input_type -> inference.GetModelStatsRequest
	11, // 11: inference.InferenceService.Health:input_type -> inference.HealthRequest
	1,  // 12: inference.InferenceService.InferBatch:output_type -> inference.InferBatchResponse
	4,  // 13: inference.InferenceService.ListModels:output_type -> inference.ListModelsResponse
	6,  // 14: inference.InferenceService.LoadModel:output_type -> inference.LoadModelResponse
	8,  // 15: inference.InferenceService.UnloadModel:output_type -> inference.UnloadModelResponse
	10, // 16: inference.InferenceService.GetModelStats:output_type -> inference.GetModelStatsResponse
	12, // 17: inference.InferenceService.Health:output_type -> inference.HealthResponse
	12, // [12:18] is the sub-list for method output_type
	6,  // [6:12] is the sub-list for method input_type
	6,  // [6:6] is the sub-list for extension type_name
	6,  // [6:6] is the sub-list for extension extendee
	0,  // [0:6] is the sub-list for field type_name
}

func init() { file_proto_inference_proto_init() }
func file_proto_inference_proto_init() {
	if File_proto_inference_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_proto_inference_proto_rawDesc), len(file_proto_inference_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   16,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_proto_inference_proto_goTypes,
		DependencyIndexes: file_proto_inference_proto_depIdxs,
		MessageInfos:      file_proto_inference_proto_msgTypes,
	}.Build()
	File_proto_inference_proto = out.File
	file_proto_inference_proto_goTypes = nil
	file_proto_inference_proto_depIdxs = nil
}
