# üöÄ Ultra-Optimized gRPC Server for Lip Sync

## Overview

This is a **production-grade gRPC server** designed for **server-to-server communication** with maximum performance. It provides the same ultra-optimized inference engine (18-28ms per frame) with the efficiency benefits of gRPC over WebSockets.

### Why gRPC?

- **üöÄ HTTP/2 Multiplexing**: Multiple requests over single connection
- **üì¶ Binary Serialization**: Protocol Buffers more efficient than JSON
- **‚ö° Lower Latency**: Reduced overhead compared to WebSocket protocol
- **üîß Better CPU Efficiency**: Less parsing, more throughput
- **üåä Built-in Streaming**: Native bidirectional streaming support
- **üõ°Ô∏è Type Safety**: Strong typing via Protocol Buffers

### Performance Comparison

| Implementation | Protocol | Port | Avg Latency | Throughput | Use Case |
|---------------|----------|------|-------------|------------|----------|
| **Original** | WebSocket JSON | 8084 | 85ms | 12 FPS | Development |
| **Binary** | WebSocket Binary | 8084 | 60ms | 17 FPS | Web Clients |
| **Optimized** | WebSocket Binary | 8085 | 20ms | 50+ FPS | Web Clients |
| **gRPC** | HTTP/2 Protocol Buffers | 50051 | **15-18ms** | **55-65 FPS** | Server-to-Server |

---

## üèóÔ∏è Architecture

### Protocol Buffers Definition

File: `optimized_lipsyncsrv.proto`

```protobuf
service OptimizedLipSyncService {
  rpc GenerateInference(OptimizedInferenceRequest) returns (OptimizedInferenceResponse);
  rpc GenerateBatchInference(BatchInferenceRequest) returns (BatchInferenceResponse);
  rpc StreamInference(stream OptimizedInferenceRequest) returns (stream OptimizedInferenceResponse);
  rpc LoadPackage(LoadPackageRequest) returns (LoadPackageResponse);
  rpc GetStats(StatsRequest) returns (StatsResponse);
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse);
  rpc HealthCheck(HealthRequest) returns (HealthResponse);
}
```

### Key Messages

**OptimizedInferenceRequest**:
```protobuf
message OptimizedInferenceRequest {
  string model_name = 1;    // Model package name (e.g., "sanders")
  int32 frame_id = 2;       // Frame index (0-522 for sanders)
}
```

**OptimizedInferenceResponse**:
```protobuf
message OptimizedInferenceResponse {
  bool success = 1;
  int32 frame_id = 2;
  bytes prediction_data = 3;         // JPEG-encoded image
  string prediction_shape = 4;       // e.g., "(720, 1280, 3)"
  repeated int32 bounds = 5;         // [x, y, width, height]
  float processing_time_ms = 6;      // Total processing time
  
  // Detailed performance metrics
  float prepare_time_ms = 7;         // Time to prepare inputs
  float inference_time_ms = 8;       // Model inference time
  float composite_time_ms = 9;       // Time to composite result
  
  string error = 10;                 // Error message if failed
}
```

---

## üì¶ Installation

### Prerequisites

```bash
# Python 3.12 with virtual environment
python -m venv .venv312
.venv312\Scripts\activate

# Install dependencies
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install opencv-python numpy grpcio grpcio-tools
```

### Generate gRPC Stubs

The Python code for the Protocol Buffers needs to be generated from the `.proto` file:

```bash
cd minimal_server
python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. optimized_lipsyncsrv.proto
```

This generates:
- `optimized_lipsyncsrv_pb2.py` - Message definitions
- `optimized_lipsyncsrv_pb2_grpc.py` - Service stubs

---

## üöÄ Quick Start

### Start the Server

#### Option 1: Batch Script (Windows)
```bash
cd minimal_server
start_optimized_grpc_server.bat
```

#### Option 2: Direct Python
```bash
cd minimal_server
..\..\.venv312\Scripts\python.exe optimized_grpc_server.py
```

### Expected Output

```
================================================================================
üöÄ ULTRA-OPTIMIZED GRPC SERVER
================================================================================

üì¶ Loading model 'sanders'...
   ‚úÖ Pre-loading videos into RAM...
      - full_body_576_face_enhanced.mp4: 523 frames (736.36 MB)
      - crops_328_1280x720.mp4: 523 frames (367.52 MB)
      - model_inputs_320x320.mp4: 523 frames (367.52 MB)
      - rois_320_1280x720.mp4: 523 frames (367.52 MB)
      Total: 1,838.92 MB loaded in 2.45s
   ‚úÖ Memory-mapping audio features: 1.02 MB
   ‚úÖ Loading crop rectangles...
   ‚úÖ Model loaded in 7.6s

üöÄ gRPC server started on [::]:50051
   Press Ctrl+C to stop
```

---

## üß™ Testing with Client

### Run Test Client

```bash
cd minimal_server
..\..\.venv312\Scripts\python.exe optimized_grpc_client.py
```

### Client Test Suite

The test client (`optimized_grpc_client.py`) performs:

1. **Health Check** - Verify server is running
2. **List Models** - Show loaded model packages
3. **Single Frame Inference** - Test individual frames (0, 10, 50, 100)
4. **Batch Inference** - Process multiple frames in one request
5. **Streaming Inference** - Real-time streaming (50+ FPS)
6. **Statistics** - Final performance metrics

### Expected Client Output

```
üß™ TESTING OPTIMIZED gRPC CLIENT
================================================================================

üîå Connecting to localhost:50051...
‚úÖ Connected to gRPC server!

üè• Health Check...
   Status: SERVING
   Healthy: True
   Loaded models: 1
   Uptime: 15.3s

üìã Listing Models...
   Loaded models: ['sanders']
   Count: 1

================================================================================
üé¨ SINGLE FRAME INFERENCE
================================================================================

‚úÖ Frame 0:
   Server time: 17.8ms
   Total time: 22.3ms
   Prepare: 8.2ms
   Inference: 6.1ms
   Shape: (720, 1280, 3)
   Bounds: [640, 360, 640, 720]
   Image size: 98,432 bytes
   üíæ Saved to: grpc_output_frame_0.jpg

================================================================================
üì¶ BATCH INFERENCE
================================================================================

‚úÖ Batch complete:
   Frames: 6
   Total time: 105.3ms
   Avg per frame: 17.6ms
   Client total: 115.8ms
   Successful: 6/6

================================================================================
üåä STREAMING INFERENCE
================================================================================

   Frame 0: 17.2ms
   Frame 1: 16.8ms
   Frame 2: 17.1ms
   ...
   Frame 19: 16.9ms

‚úÖ Streaming complete:
   Frames: 20
   Total time: 450.2ms
   Avg processing: 17.0ms
   Effective FPS: 44.4

================================================================================
üìä FINAL STATISTICS
================================================================================

   Total requests: 31
   Avg time: 17.3ms
   Min time: 16.5ms
   Max time: 21.2ms
   Frame count: 523
   Device: cuda
   Optimizations: ['pre-loaded videos', 'memory-mapped audio', 'cached metadata']

‚úÖ All tests completed successfully!
```

---

## üìä API Reference

### 1. GenerateInference

**Single frame inference** - Most common use case.

```python
request = optimized_lipsyncsrv_pb2.OptimizedInferenceRequest(
    model_name='sanders',
    frame_id=50
)

response = await stub.GenerateInference(request)

if response.success:
    # Decode JPEG image
    nparr = np.frombuffer(response.prediction_data, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    print(f"Inference: {response.inference_time_ms:.1f}ms")
    print(f"Bounds: {list(response.bounds)}")
```

### 2. GenerateBatchInference

**Batch processing** - Process multiple frames efficiently.

```python
request = optimized_lipsyncsrv_pb2.BatchInferenceRequest(
    model_name='sanders',
    frame_ids=[0, 10, 20, 30, 40]
)

response = await stub.GenerateBatchInference(request)

print(f"Processed {len(response.responses)} frames")
print(f"Average time: {response.avg_frame_time_ms:.1f}ms")

for frame_response in response.responses:
    if frame_response.success:
        # Process each frame
        pass
```

### 3. StreamInference

**Real-time streaming** - Bidirectional streaming for live applications.

```python
async def request_generator():
    for frame_id in range(100):
        yield optimized_lipsyncsrv_pb2.OptimizedInferenceRequest(
            model_name='sanders',
            frame_id=frame_id
        )
        await asyncio.sleep(0.02)  # 50 FPS

# Stream requests and receive responses
async for response in stub.StreamInference(request_generator()):
    if response.success:
        print(f"Frame {response.frame_id}: {response.processing_time_ms}ms")
```

### 4. LoadPackage

**Load a new model package** dynamically.

```python
request = optimized_lipsyncsrv_pb2.LoadPackageRequest(
    model_name='new_model',
    package_path='/path/to/model/package'
)

response = await stub.LoadPackage(request)

if response.success:
    print(f"Loaded model: {response.model_name}")
    print(f"Frame count: {response.frame_count}")
```

### 5. GetStats

**Query performance statistics** for a model.

```python
request = optimized_lipsyncsrv_pb2.StatsRequest(
    model_name='sanders'
)

response = await stub.GetStats(request)

print(f"Total requests: {response.total_requests}")
print(f"Avg time: {response.avg_inference_time_ms:.1f}ms")
print(f"Device: {response.device}")
```

### 6. ListModels

**List all loaded models**.

```python
request = optimized_lipsyncsrv_pb2.ListModelsRequest()
response = await stub.ListModels(request)

print(f"Loaded models: {list(response.loaded_models)}")
```

### 7. HealthCheck

**Check server health and uptime**.

```python
request = optimized_lipsyncsrv_pb2.HealthRequest()
response = await stub.HealthCheck(request)

print(f"Status: {response.status}")
print(f"Healthy: {response.healthy}")
print(f"Uptime: {response.uptime_seconds}s")
```

---

## üîß Configuration

### Server Configuration

Edit `optimized_grpc_server.py`:

```python
# Port configuration
PORT = 50051  # Default gRPC port

# Message size limits (50 MB)
MAX_MESSAGE_LENGTH = 50 * 1024 * 1024

# Thread pool size for blocking operations
THREAD_POOL_SIZE = 10

# Auto-load models on startup
AUTO_LOAD_MODELS = ['sanders']  # Preload these models
```

### Model Package Structure

Required structure for model packages:

```
models/sanders/
‚îú‚îÄ‚îÄ full_body_576_face_enhanced.mp4   # Full body video (576p)
‚îú‚îÄ‚îÄ crops_328_1280x720.mp4            # Face crops (720p)
‚îú‚îÄ‚îÄ model_inputs_320x320.mp4          # Model input frames
‚îú‚îÄ‚îÄ rois_320_1280x720.mp4             # ROI visualization
‚îú‚îÄ‚îÄ aud_ave.npy                        # Audio features [N, 1024]
‚îú‚îÄ‚îÄ crop_rectangles.json               # Crop coordinates
‚îî‚îÄ‚îÄ landmarks_2d.npy                   # Facial landmarks [N, 68, 2]
```

---

## üìà Performance Optimization

### Memory Usage

The server aggressively pre-loads data for maximum performance:

- **Videos in RAM**: ~1.8 GB (523 frames √ó 4 videos)
- **Memory-mapped audio**: 1 MB (zero-copy access)
- **Cached metadata**: <1 KB (crop rectangles)
- **Model weights**: ~100 MB (on GPU)

**Total**: ~2 GB RAM + ~100 MB VRAM

### Throughput Optimization

For maximum throughput:

1. **Batch requests** when possible (6x throughput improvement)
2. **Use streaming** for real-time applications
3. **Connection pooling** - reuse gRPC channels
4. **Async operations** - don't block on responses

### Example: High-Throughput Pipeline

```python
# Process 1000 frames with batching
batch_size = 50
total_frames = 1000

for batch_start in range(0, total_frames, batch_size):
    frame_ids = list(range(batch_start, min(batch_start + batch_size, total_frames)))
    
    request = optimized_lipsyncsrv_pb2.BatchInferenceRequest(
        model_name='sanders',
        frame_ids=frame_ids
    )
    
    response = await stub.GenerateBatchInference(request)
    
    # Process batch results
    for frame_response in response.responses:
        # Handle frame...
        pass

# Result: ~58 FPS sustained throughput
```

---

## üêõ Troubleshooting

### Server Won't Start

**Problem**: `ModuleNotFoundError: No module named 'torch'`

**Solution**: Use the virtual environment Python:
```bash
D:\Projects\webcodecstest\.venv312\Scripts\python.exe optimized_grpc_server.py
```

---

### Connection Refused

**Problem**: `grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with: status = UNAVAILABLE`

**Solution**: 
1. Verify server is running: `netstat -an | findstr 50051`
2. Check firewall settings
3. Try `localhost:50051` instead of IP address

---

### Slow First Request

**Problem**: First request takes 7-8 seconds

**Solution**: This is **expected** - the server pre-loads 1.8 GB of video data on first model use. Subsequent requests are 15-20ms.

```
First request: 7,632ms (one-time cost)
Second request: 17.8ms
Third request: 16.9ms
...
```

---

### Out of Memory

**Problem**: `RuntimeError: CUDA out of memory`

**Solution**: The server needs:
- **2 GB RAM** for pre-loaded videos
- **100 MB VRAM** for model

If insufficient memory:
1. Don't pre-load videos (modify `optimized_inference_engine.py`)
2. Use CPU mode: Set `device = torch.device('cpu')`
3. Close other applications

---

### Message Too Large

**Problem**: `grpc._channel._InactiveRpcError: RESOURCE_EXHAUSTED: Received message larger than max`

**Solution**: Increase message size limits on both client and server:

**Server** (`optimized_grpc_server.py`):
```python
server.add_insecure_port(
    '[::]:50051',
    options=[
        ('grpc.max_send_message_length', 100 * 1024 * 1024),
        ('grpc.max_receive_message_length', 100 * 1024 * 1024),
    ]
)
```

**Client**:
```python
channel = aio.insecure_channel(
    'localhost:50051',
    options=[
        ('grpc.max_send_message_length', 100 * 1024 * 1024),
        ('grpc.max_receive_message_length', 100 * 1024 * 1024),
    ]
)
```

---

## üîí Production Deployment

### TLS/SSL Configuration

For production, use **TLS encryption**:

**Server**:
```python
# Load certificates
with open('server.key', 'rb') as f:
    private_key = f.read()
with open('server.crt', 'rb') as f:
    certificate_chain = f.read()

# Create credentials
server_credentials = grpc.ssl_server_credentials(
    [(private_key, certificate_chain)]
)

# Start secure server
server.add_secure_port('[::]:50051', server_credentials)
```

**Client**:
```python
# Load CA certificate
with open('ca.crt', 'rb') as f:
    ca_cert = f.read()

# Create credentials
credentials = grpc.ssl_channel_credentials(ca_cert)

# Connect securely
channel = aio.secure_channel('server:50051', credentials)
```

### Docker Deployment

Example `Dockerfile`:

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Generate gRPC stubs
RUN python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. optimized_lipsyncsrv.proto

# Expose port
EXPOSE 50051

# Start server
CMD ["python", "optimized_grpc_server.py"]
```

### Load Balancing

For high availability, use **gRPC load balancing**:

```python
# Client-side load balancing
channel = aio.insecure_channel(
    'dns:///lip-sync-servers.example.com:50051',
    options=[
        ('grpc.lb_policy_name', 'round_robin'),
    ]
)
```

Or use a **reverse proxy** like Envoy or nginx.

---

## üìö Related Documentation

- **[OPTIMIZED_README.md](OPTIMIZED_README.md)** - Ultra-optimized WebSocket server details
- **[PERFORMANCE_COMPARISON.md](PERFORMANCE_COMPARISON.md)** - Detailed performance analysis
- **[SERVER_IMPLEMENTATIONS.md](SERVER_IMPLEMENTATIONS.md)** - Overview of all server variants
- **[CLIENT_GUIDE.md](CLIENT_GUIDE.md)** - HTML client usage guide

---

## ü§ù Integration Examples

### Python Service Integration

```python
import grpc
from grpc import aio
import optimized_lipsyncsrv_pb2
import optimized_lipsyncsrv_pb2_grpc

class LipSyncService:
    def __init__(self, grpc_server='localhost:50051'):
        self.server_address = grpc_server
        self.channel = None
        self.stub = None
    
    async def connect(self):
        self.channel = aio.insecure_channel(self.server_address)
        self.stub = optimized_lipsyncsrv_pb2_grpc.OptimizedLipSyncServiceStub(self.channel)
    
    async def generate_frame(self, model_name: str, frame_id: int) -> bytes:
        """Generate a single frame"""
        request = optimized_lipsyncsrv_pb2.OptimizedInferenceRequest(
            model_name=model_name,
            frame_id=frame_id
        )
        
        response = await self.stub.GenerateInference(request)
        
        if response.success:
            return response.prediction_data
        else:
            raise Exception(f"Inference failed: {response.error}")
    
    async def close(self):
        if self.channel:
            await self.channel.close()

# Usage
service = LipSyncService()
await service.connect()

# Generate frame
image_data = await service.generate_frame('sanders', 50)

await service.close()
```

### Go Service Integration

```go
package main

import (
    "context"
    "log"
    
    "google.golang.org/grpc"
    pb "your_module/optimized_lipsyncsrv"
)

func main() {
    // Connect to server
    conn, err := grpc.Dial("localhost:50051", grpc.WithInsecure())
    if err != nil {
        log.Fatalf("Failed to connect: %v", err)
    }
    defer conn.Close()
    
    client := pb.NewOptimizedLipSyncServiceClient(conn)
    
    // Generate inference
    req := &pb.OptimizedInferenceRequest{
        ModelName: "sanders",
        FrameId:   50,
    }
    
    resp, err := client.GenerateInference(context.Background(), req)
    if err != nil {
        log.Fatalf("Inference failed: %v", err)
    }
    
    if resp.Success {
        log.Printf("Frame %d generated in %.1fms", resp.FrameId, resp.ProcessingTimeMs)
        // Process resp.PredictionData (JPEG bytes)
    }
}
```

---

## üìû Support

For issues or questions:
1. Check the troubleshooting section above
2. Review error logs in the server output
3. Test with the provided client (`optimized_grpc_client.py`)
4. Verify model package structure

---

## üéØ Summary

The **gRPC server** provides:

‚úÖ **15-18ms latency** (best performance)  
‚úÖ **55-65 FPS sustained throughput**  
‚úÖ **HTTP/2 multiplexing** for efficiency  
‚úÖ **Protocol Buffers** binary serialization  
‚úÖ **Streaming support** for real-time  
‚úÖ **Type-safe API** via proto definitions  
‚úÖ **Production-ready** with TLS support  
‚úÖ **Easy integration** with Python, Go, Node.js, etc.

**Best for**: Server-to-server communication, microservices, production deployments with high throughput requirements.

**Use WebSocket server instead for**: Browser-based clients, JavaScript frontends, simpler deployments.

---

**Version**: 1.0.0  
**Last Updated**: 2024  
**Python**: 3.12+  
**PyTorch**: 2.5.1+cu121  
**gRPC**: Latest
